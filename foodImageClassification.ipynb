{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import cv2\n",
    "from torchinfo import summary\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "DATA_DIR = 'combinedDataset'\n",
    "BATCH_SIZE = 16\n",
    "SEED = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m         name \u001b[39m=\u001b[39m name[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m name\n\u001b[1;32m---> 21\u001b[0m train_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(get_file_paths(DATA_DIR), columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     22\u001b[0m train_df[\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(get_class_name)\n\u001b[0;32m     23\u001b[0m train_df\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def get_file_paths(dir):\n",
    "\n",
    "    path_list = os.listdir(dir)\n",
    "    files_list = list()\n",
    "    for entry in path_list:\n",
    "        path = os.path.join(dir, entry)\n",
    "        if os.path.isdir(path):\n",
    "            files_list = files_list + get_file_paths(path)\n",
    "        else:\n",
    "            files_list.append(path)\n",
    "    return sorted(files_list)\n",
    "\n",
    "def get_class_name(path):\n",
    " \n",
    "    name = path.split('\\\\')[1]\n",
    "    name = name.upper()\n",
    "    if name[-1] == 'S':\n",
    "        name = name[:-1]\n",
    "    return name\n",
    "\n",
    "train_df = pd.DataFrame(get_file_paths(DATA_DIR), columns=['path'])\n",
    "train_df['class'] = train_df['path'].apply(get_class_name)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(train_df['class'].unique())\n",
    "idx_to_class = {i:j for i, j in enumerate(classes)}\n",
    "class_to_idx = {value:key for key,value in idx_to_class.items()}\n",
    "\n",
    "train_df['class'] = train_df['class'].apply(lambda x: class_to_idx[x])\n",
    "\n",
    "train_df, test_df = train_test_split(train_df, test_size=0.2, random_state=SEED)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=SEED)\n",
    "\n",
    "class Dataset_Food(Dataset):\n",
    "\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "       \n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "      \n",
    "        image_filepath = self.df.loc[i, 'path']\n",
    "        \n",
    "        image = cv2.imread(image_filepath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        label = self.df.loc[i, 'class']\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)['image']\n",
    "        \n",
    "        return image, label, image_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = A.Compose(\n",
    "    [\n",
    "        A.RandomResizedCrop(height=224, width=224, scale=(0.8, 1.2), ratio=(0.8, 1.2), p=0.8),\n",
    "        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0, rotate_limit=20, p=0.8, border_mode=0),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.MedianBlur(blur_limit=7, p=0.5),\n",
    "        A.LongestMaxSize(224),\n",
    "        A.PadIfNeeded(224, 224),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_test = A.Compose(\n",
    "        [\n",
    "          A.LongestMaxSize(224),\n",
    "          A.PadIfNeeded(224, 224),\n",
    "          A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "          ToTensorV2()\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset_Food' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[39m=\u001b[39m Dataset_Food(train_df, transforms)\n\u001b[0;32m      2\u001b[0m val_dataset \u001b[39m=\u001b[39m Dataset_Food(val_df, transforms_test)\n\u001b[0;32m      3\u001b[0m test_dataset \u001b[39m=\u001b[39m Dataset_Food(test_df, transforms_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset_Food' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset_Food(train_df, transforms)\n",
    "val_dataset = Dataset_Food(val_df, transforms_test)\n",
    "test_dataset = Dataset_Food(test_df, transforms_test)\n",
    "\n",
    "plot_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "images = []\n",
    "\n",
    "for i in range(9):\n",
    "    image = next(iter(plot_loader))[0][0]\n",
    "    image = image.permute(1, 2, 0)\n",
    "    image = image * torch.tensor([0.229, 0.224, 0.225])\n",
    "    image = image + torch.tensor([0.485, 0.456, 0.406])\n",
    "    images.append(image)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(8, 8))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        ax[i][j].imshow(images[3 * i + j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_CLASS = 55\n",
    "TRY = \"Try6\"\n",
    "FOLDER = 'C:\\\\Users\\Somya\\\\Downloads\\\\food\\\\food\\\\' + TRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\somagarw\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdensenet\u001b[39;00m \u001b[39mimport\u001b[39;00m densenet121\n\u001b[1;32m----> 3\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mModel\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      4\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m    Neural network model based on DenseNet\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, in_channels\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, num_classes\u001b[39m=\u001b[39mNUM_CLASS):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.models.densenet import densenet121\n",
    "\n",
    "class Model(nn.Module):\n",
    "    '''\n",
    "    Neural network model based on DenseNet\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Input channels\n",
    "        num_classes (int): Output channels\n",
    "\n",
    "    '''\n",
    "    def __init__(self, in_channels=3, num_classes=NUM_CLASS):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.densenet = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', weights='IMAGENET1K_V1')\n",
    "        #self.densenet = densenet121()\n",
    "        #self.densenet = DenseNet.from_pretrained('densenet121', num_classes=num_classes)\n",
    "        self.densenet.classifier = nn.Linear(in_features=1920, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device)\n",
    "\n",
    "summary(model, (BATCH_SIZE, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningAverage():\n",
    "    '''\n",
    "    Class to save and update the running average \n",
    "\n",
    "    '''    \n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.total = 0\n",
    "    \n",
    "    def update(self, n):\n",
    "        '''\n",
    "        Updates running average with new value\n",
    "\n",
    "        Args:\n",
    "            n : value to add to the running average\n",
    "\n",
    "        Returns:\n",
    "            files_list (list): List of paths for each file\n",
    "        '''\n",
    "        self.total += n\n",
    "        self.count += 1\n",
    "    \n",
    "    def __call__(self):\n",
    "        '''\n",
    "        Returns current running average\n",
    "\n",
    "        Returns:\n",
    "            (float): Running average\n",
    "        '''\n",
    "        return self.total/(self.count + 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, val_loader, epochs, device, lr_scheduler, scaler):#, state, filename):\n",
    "    '''\n",
    "    Trains and evaluates the model over defined number of epochs\n",
    "    \n",
    "    Args:\n",
    "        model: Defined model\n",
    "        optimizer: Constructed optimizer\n",
    "        criterion: Loss function\n",
    "        train_loader: Training dataset iterable\n",
    "        val_loader: Validation dataset iterable\n",
    "        epochs: Number of epochs\n",
    "        device: Device for tensor storage\n",
    "        lr_scheduler: Learning rate scheduler\n",
    "        state: Parameters for model saving\n",
    "        filename: Path for saved model\n",
    "    '''\n",
    "    val_losses = []\n",
    "    val_accu = []\n",
    "    train_loss = []\n",
    "    train_accu = []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        acc_avg = RunningAverage()\n",
    "        loss_avg = RunningAverage()\n",
    "        val_acc_avg = RunningAverage()\n",
    "        val_loss_avg = RunningAverage()\n",
    "        best_val_acc = 0\n",
    "\n",
    "        model.train()\n",
    "        with tqdm(total=len(train_loader), leave=False, file=sys.stdout) as t:\n",
    "            t.set_description(f'Epoch {e + 1}, LR {lr_scheduler.get_last_lr()[0]:.5f}')\n",
    "            for train_batch, labels_batch, file_path in train_loader:\n",
    "                train_batch, labels_batch = train_batch.to(device), labels_batch.to(device)\n",
    "\n",
    "                with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
    "                    output_batch = model(train_batch)\n",
    "                    loss = criterion(output_batch, labels_batch)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "                # optimizer.zero_grad()\n",
    "                # loss.backward()\n",
    "                # optimizer.step()\n",
    "                \n",
    "                predicted = torch.argmax(output_batch, 1)\n",
    "                accuracy_batch = torch.sum(predicted == labels_batch) / labels_batch.shape[0]\n",
    "                acc_avg.update(accuracy_batch)\n",
    "                loss_avg.update(loss.item())\n",
    "                \n",
    "                t.set_postfix({'stats': f'train_loss: {loss_avg():.4f}, train_acc: {acc_avg():.4f}'})\n",
    "                t.update()\n",
    "\n",
    "        epoch_train_stats = f'Epoch {e + 1}. LR {lr_scheduler.get_last_lr()[0]:.5f}, train_loss: {loss_avg():.4f}, train_acc: {acc_avg():.4f},'\n",
    "        train_loss.append(loss_avg())\n",
    "        train_accu.append(acc_avg())\n",
    "\n",
    "        lr_scheduler.step()\n",
    "                \n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            for val_batch, val_labels_batch, file_path in val_loader:\n",
    "                val_batch, val_labels_batch = val_batch.to(device), val_labels_batch.to(device)\n",
    "\n",
    "                val_output_batch = model(val_batch)\n",
    "                val_loss = criterion(val_output_batch, val_labels_batch)\n",
    "\n",
    "                val_predicted = torch.argmax(val_output_batch, 1) \n",
    "                val_accuracy_batch = torch.sum(val_predicted == val_labels_batch) / val_labels_batch.shape[0]\n",
    "                val_acc_avg.update(val_accuracy_batch)\n",
    "                val_loss_avg.update(loss.item())\n",
    "\n",
    "        print(f'{epoch_train_stats} val_loss: {val_loss_avg():.4f}, val_acc: {val_acc_avg():.4f}', sep='')\n",
    "        val_losses.append(val_loss_avg())\n",
    "        val_accu.append(val_acc_avg())\n",
    "        # if val_acc_avg() > best_val_acc:\n",
    "        #     torch.save(state, filename)\n",
    "        #     best_val_acc = val_acc_avg()\n",
    "\n",
    "\n",
    "    with open(FOLDER + '\\\\train_loss.txt', 'w') as fp:\n",
    "        fp.write('\\n'.join('%s ' % x for x in train_loss))\n",
    "\n",
    "    with open(FOLDER + '\\\\val_loss.txt', 'w') as fp:\n",
    "        fp.write('\\n'.join('%s ' % x for x in val_losses))\n",
    "\n",
    "    with open(FOLDER + '\\\\train_accu.txt', 'w') as fp:\n",
    "        fp.write('\\n'.join('%s ' % x for x in train_accu))\n",
    "\n",
    "    with open(FOLDER + '\\\\val_accu.txt', 'w') as fp:\n",
    "        fp.write('\\n'.join('%s ' % x for x in val_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "#optimizer = torch.optim.Adamax(model.parameters(), lr=LEARNING_RATE)\n",
    "#lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=0.00001, last_epoch=-1)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, last_epoch=-1)\n",
    "use_amp = True\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "#checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "#path = 'model.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scheduler(epochs, lr, lr_scheduler, **kwargs):\n",
    "    optimizer = torch.optim.Adam([torch.tensor(1)], lr=lr)\n",
    "    lr_scheduler = lr_scheduler(optimizer, **kwargs)\n",
    "\n",
    "    x = [i + 1 for i in range(epochs)]\n",
    "    y = []\n",
    "    for i in range(epochs):\n",
    "        optimizer.step()\n",
    "        y.append(lr_scheduler.get_last_lr())\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,5))\n",
    "    ax.plot(x, y)\n",
    "    ax.set_title('Learning rate schedule')\n",
    "    ax.set_ylabel('Learning rate')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.xaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "    \n",
    "#plot_scheduler(EPOCHS, LEARNING_RATE, torch.optim.lr_scheduler.CosineAnnealingLR, T_max=EPOCHS, eta_min=0.00001);\n",
    "plot_scheduler(EPOCHS, LEARNING_RATE, torch.optim.lr_scheduler.ConstantLR);\n",
    "#torch.optim.lr_scheduler.ConstantLR(optimizer, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train(model, optimizer, criterion, train_loader, val_loader, EPOCHS, device, lr_scheduler, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "test_acc = RunningAverage()\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    with open (FOLDER + '\\\\predicted.txt', 'w') as file:\n",
    "        for test_batch, test_labels_batch, file_path in test_loader:\n",
    "                    test_batch, test_labels_batch = test_batch.to(device), test_labels_batch.to(device)\n",
    "                    test_output_batch = model(test_batch)\n",
    "                    test_predicted = torch.argmax(test_output_batch, 1)\n",
    "                    test_accuracy_batch = torch.sum(test_predicted == test_labels_batch) / test_labels_batch.shape[0]\n",
    "                    test_acc.update(test_accuracy_batch)\n",
    "\n",
    "                    #_,prediction = test_output_batch.max(5)\n",
    "                    _,prediction = test_output_batch.topk(5,dim=1)\n",
    "                    predicted_classes = prediction[0].tolist()\n",
    "\n",
    "                    #print(\"Top 5 predicted class numbers:\", predicted_classes)\n",
    "\n",
    "\n",
    "                    str = \"\"\n",
    "                    for i in predicted_classes:\n",
    "\n",
    "                        str += \",\" + idx_to_class[i]\n",
    "                        #print(str + \" \")\n",
    "\n",
    "                    print(file_path)\n",
    "                    file_data = file_path[0] + \",\" + get_class_name(file_path[0])\n",
    "                    file.write(file_data)\n",
    "                    file.write(str)\n",
    "                    file.write(\"\\n\")\n",
    "                        #print(prediction.item())\n",
    "                    #str = idx_to_class[prediction.item()]\n",
    "                    #print(str)\n",
    "print(f'Test dataset accuracy: {test_acc():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
